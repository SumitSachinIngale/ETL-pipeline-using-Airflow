# ETL-pipeline-using-Airflow
Everyday a DAG will be triggered by airflow which will clean the data , dumped in given location by using pandas and then aggregation will be perfomed using sql and output will be stored in given output directory , if the dag is success then email will be send .
